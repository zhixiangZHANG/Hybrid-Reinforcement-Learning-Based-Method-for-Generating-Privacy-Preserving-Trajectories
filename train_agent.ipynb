{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb98a58-f555-427d-b464-f8a1dd30d440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete, MultiBinary\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "# import scipy\n",
    "# import scipy.stats\n",
    "from my_function import save_variable, load_variable, cor_to_ind, ind_to_cor, cor_to_nor, nor_to_cor, lat_to_cor, a_star_search\n",
    "from my_ppo import PPO, Memory, ActorCritic, Critic, Actor\n",
    "from my_env import MyEnv\n",
    "log_interval = 1000  # print avg reward in the interval\n",
    "max_timesteps = 300  # max timesteps in one episode\n",
    "update_timestep = 300*30  # update policy every n timesteps 800*24 (max memory size)\n",
    "action_var = [0.2, 0.2]  # constant variance for action distribution (Multivariate Normal)\n",
    "K_epochs = 40  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "gamma = 0.99  # discount factor\n",
    "lr = 0.00005  # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "random_seed = 3\n",
    "train_env = MyEnv(train = True)\n",
    "state_dim = 32\n",
    "action_dim = train_env.action_space.shape[0]\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "train_memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "# ppo = load_variable('ppo_best') # \n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "time_step = 0\n",
    "update_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ba59b-72ca-4fa4-96ca-8a6d14806206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 \t AvgR: 137.55\n",
      "Episode: 2000 \t AvgR: 243.68\n",
      "Episode: 3000 \t AvgR: 131.50\n",
      "Episode: 4000 \t AvgR: 406.87\n",
      "Episode: 5000 \t AvgR: -22.77\n",
      "Episode: 6000 \t AvgR: 231.17\n",
      "Episode: 7000 \t AvgR: 366.70\n",
      "Episode: 8000 \t AvgR: 391.22\n",
      "Episode: 9000 \t AvgR: 484.34\n",
      "Episode: 10000 \t AvgR: 504.02\n",
      "Episode: 11000 \t AvgR: 162.79\n",
      "Episode: 12000 \t AvgR: 423.17\n",
      "Episode: 13000 \t AvgR: 241.85\n",
      "Episode: 14000 \t AvgR: 147.90\n",
      "Episode: 15000 \t AvgR: 135.19\n",
      "Episode: 16000 \t AvgR: 520.55\n",
      "Episode: 17000 \t AvgR: 516.25\n"
     ]
    }
   ],
   "source": [
    "train_df = load_variable('train_df')\n",
    "test_df = load_variable('test_df')\n",
    "columns = ['Episode', 'AvgLen', 'AvgR', 'AvgV', 'Speedvar','Succ_Rate','Test_time', 'Train_time']\n",
    "test_result = pd.DataFrame(columns=columns)\n",
    "T1= time.perf_counter()\n",
    "\n",
    "save_reward = []\n",
    "save_speed = []\n",
    "save_episode = []\n",
    "save_length = []\n",
    "save_speedvar = []\n",
    "last_reward = 0\n",
    "\n",
    "for i_episode in range(0, len(train_df)):\n",
    "    state_1, state_2 = train_env.reset()\n",
    "    train_temp_rewards = 0\n",
    "    for t in range(max_timesteps):\n",
    "        time_step += 1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state_1, state_2, train_memory)\n",
    "        state_1, state_2, reward, done= train_env.step(action)\n",
    "        train_temp_rewards += reward\n",
    "        # Saving reward and is_terminals:\n",
    "        train_memory.rewards.append(reward)\n",
    "        train_memory.is_terminals.append(done)\n",
    "        # if time_step % update_timestep == 0:\n",
    "        #     flag = True\n",
    "        # if flag:\n",
    "        #     if t == max_timesteps-1 or done:\n",
    "        #         ppo.update(train_memory)\n",
    "        #         train_memory.clear_memory()\n",
    "        #         time_step = 0\n",
    "        #         flag = False\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(train_memory)\n",
    "            train_memory.clear_memory()\n",
    "            time_step = 0\n",
    "        if done:\n",
    "            break        \n",
    "    \n",
    "    if (i_episode+1) % log_interval == 0:\n",
    "        path = 'ppoF/ppo' + str(i_episode+1)\n",
    "        print('Episode: {} \\t AvgR: {:.2f}'.format(i_episode+1, train_temp_rewards))\n",
    "        save_variable(ppo, path)\n",
    "        # T2 = time.perf_counter()\n",
    "        # test_env = MyEnv(train = False)\n",
    "        # test_memory = Memory()\n",
    "        # speeds = []\n",
    "        # lengths = []\n",
    "        # rewards = []\n",
    "        # test_start_time = time.perf_counter()\n",
    "        # succes_num = 0\n",
    "        # for j in range(0, 200):\n",
    "        #     state_1, state_2 = test_env.reset()\n",
    "        #     reward = 0\n",
    "        #     for j_t in range(500):\n",
    "        #         action = ppo.select_action(state_1, state_2, test_memory) # 在计算action时，把state, action, logprobs存入 memory\n",
    "        #         state_1, state_2, reward, done= test_env.step(action)\n",
    "        #         reward += reward\n",
    "        #         speed = np.linalg.norm(state_1[2:4]*5, ord = 2)\n",
    "        #         speeds.append(speed)\n",
    "        #         if done:\n",
    "        #             succes_num += 1\n",
    "        #             break\n",
    "        #     test_memory.clear_memory()\n",
    "        #     lengths.append(j_t+1)\n",
    "        #     rewards.append(reward)\n",
    "        # test_end_time = time.perf_counter()\n",
    "        # avg_length = np.mean(lengths)\n",
    "        # avg_reward = np.mean(rewards)\n",
    "        # avg_speed = np.mean(speeds)\n",
    "        # var_speed = np.var(speeds)\n",
    "        # success_rate = succes_num/(len(test_df)-300)\n",
    "        # test_time = test_end_time - test_start_time   \n",
    "        # train_time = T2-T1\n",
    "        # new_row = [i_episode+1, avg_length, avg_reward, avg_speed, var_speed,success_rate, test_time, train_time]\n",
    "        # test_result.loc[len(test_result)] = new_row\n",
    "        # T1 = time.perf_counter()\n",
    "        # print('Episode {} \\t AvgLen: {:.2f} \\t AvgR: {:.2f} \\t AvgV: {:.2f} \\t Var_V:{:.2f} \\t Succ_Rate: {:.2f} \\t Test_T: {:.2f}s \\t Train_T: {:.2f}s '.format(i_episode+1, avg_length, \n",
    "        #             avg_reward, avg_speed, var_speed, success_rate, test_time, train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e71cb-05c8-411d-acbf-1e53a6dc12a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
